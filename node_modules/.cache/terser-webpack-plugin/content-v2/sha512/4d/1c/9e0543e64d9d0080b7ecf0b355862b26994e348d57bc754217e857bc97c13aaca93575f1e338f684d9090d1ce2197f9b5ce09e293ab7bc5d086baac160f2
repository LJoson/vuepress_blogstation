{"code":"(window.webpackJsonp=window.webpackJsonp||[]).push([[24],{399:function(t,s,a){\"use strict\";a.r(s);var n=a(2),e=Object(n.a)({},(function(){var t=this,s=t.$createElement,a=t._self._c||s;return a(\"ContentSlotsDistributor\",{attrs:{\"slot-key\":t.$parent.slotKey}},[a(\"p\"),a(\"div\",{staticClass:\"table-of-contents\"},[a(\"ul\",[a(\"li\",[a(\"a\",{attrs:{href:\"#环境\"}},[t._v(\"环境\")])]),a(\"li\",[a(\"a\",{attrs:{href:\"#入门\"}},[t._v(\"入门\")])])])]),a(\"p\"),t._v(\" \"),a(\"h1\",{attrs:{id:\"引言\"}},[a(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#引言\"}},[t._v(\"#\")]),t._v(\" 引言\")]),t._v(\" \"),a(\"p\",[t._v(\"虽然之前有过相关知识学习及项目的部署经验，却都是碎片化知识，而对于原理上并没有太多的理解认识。在一个大创项目的机会下开始系统的学习相关内容。总的计划是tf入手，paddle为辅，后面会以C++为主学习实现李航博士《统计学习方法》，《算法导论》，《西瓜书》，花书以及其他入门书籍的相关算法和网络结构。\")]),t._v(\" \"),a(\"h2\",{attrs:{id:\"环境\"}},[a(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#环境\"}},[t._v(\"#\")]),t._v(\" 环境\")]),t._v(\" \"),a(\"ul\",[a(\"li\",[t._v(\"win10\")]),t._v(\" \"),a(\"li\",[t._v(\"anaconda\")]),t._v(\" \"),a(\"li\",[t._v(\"pycharm\")])]),t._v(\" \"),a(\"p\",[t._v(\"新手必备，至于tf版本是cpu还是gpu，如果电脑驱动支持cudnn和cuda就优先gpu版本。\"),a(\"a\",{attrs:{href:\"https://blog.csdn.net/qq_43743037/article/details/104242758\",target:\"_blank\",rel:\"noopener noreferrer\"}},[t._v(\"更多详细安装\"),a(\"OutboundLink\")],1)]),t._v(\" \"),a(\"h2\",{attrs:{id:\"入门\"}},[a(\"a\",{staticClass:\"header-anchor\",attrs:{href:\"#入门\"}},[t._v(\"#\")]),t._v(\" 入门\")]),t._v(\" \"),a(\"div\",{staticClass:\"language-python line-numbers-mode\"},[a(\"pre\",{pre:!0,attrs:{class:\"language-python\"}},[a(\"code\",[a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"import\")]),t._v(\" tensorflow \"),a(\"span\",{pre:!0,attrs:{class:\"token keyword\"}},[t._v(\"as\")]),t._v(\" tf\\n\"),a(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"#载入并准备好 MNIST 数据集。将样本从整数转换为浮点数：\")]),t._v(\"\\nmnist \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" tf\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"keras\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"datasets\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"mnist\\n\\n\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"x_train\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" y_train\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"x_test\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" y_test\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" mnist\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"load_data\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\"\\nx_train\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" x_test \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" x_train \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"/\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"255.0\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" x_test \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"/\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"255.0\")]),t._v(\"\\n\"),a(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"#将模型的各层堆叠起来，以搭建 tf.keras.Sequential 模型。为训练选择优化器和损失函数：\")]),t._v(\"\\nmodel \"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),t._v(\" tf\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"keras\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"models\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"Sequential\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"[\")]),t._v(\"\\n  tf\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"keras\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"layers\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"Flatten\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"input_shape\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"28\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" \"),a(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"28\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n  tf\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"keras\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"layers\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"Dense\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"128\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" activation\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),a(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v(\"'relu'\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n  tf\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"keras\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"layers\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"Dropout\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"0.2\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n  tf\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"keras\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"layers\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"Dense\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),a(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"10\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" activation\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),a(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v(\"'softmax'\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\"\\n\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"]\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\"\\n\\nmodel\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),a(\"span\",{pre:!0,attrs:{class:\"token builtin\"}},[t._v(\"compile\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"optimizer\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),a(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v(\"'adam'\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n              loss\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),a(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v(\"'sparse_categorical_crossentropy'\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\"\\n              metrics\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"[\")]),a(\"span\",{pre:!0,attrs:{class:\"token string\"}},[t._v(\"'accuracy'\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"]\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\"\\n\"),a(\"span\",{pre:!0,attrs:{class:\"token comment\"}},[t._v(\"#训练并验证模型：\")]),t._v(\"\\nmodel\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"fit\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"x_train\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" y_train\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" epochs\"),a(\"span\",{pre:!0,attrs:{class:\"token operator\"}},[t._v(\"=\")]),a(\"span\",{pre:!0,attrs:{class:\"token number\"}},[t._v(\"5\")]),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\"\\nmodel\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\".\")]),t._v(\"evaluate\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\"(\")]),t._v(\"x_test\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\",\")]),t._v(\" y_test\"),a(\"span\",{pre:!0,attrs:{class:\"token punctuation\"}},[t._v(\")\")]),t._v(\"\\n\\n\")])]),t._v(\" \"),a(\"div\",{staticClass:\"line-numbers-wrapper\"},[a(\"span\",{staticClass:\"line-number\"},[t._v(\"1\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"2\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"3\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"4\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"5\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"6\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"7\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"8\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"9\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"10\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"11\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"12\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"13\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"14\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"15\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"16\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"17\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"18\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"19\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"20\")]),a(\"br\"),a(\"span\",{staticClass:\"line-number\"},[t._v(\"21\")]),a(\"br\")])])])}),[],!1,null,null,null);s.default=e.exports}}]);","extractedComments":[]}